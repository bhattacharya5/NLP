{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1kV93f9R9EHtAcgNGRcMk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharya5/NLP/blob/main/Assignment2_NLU_KnowledgeNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Build a relation classifier that can detect a predefined class of relations, as specified in the dataset. \n",
        "#2. Create a subset of the KnowledgeNet data using sentences which contain any of the following relations: (make a subset of train.json with these relations only)\n",
        "DATE_OF_BIRTH (PER–DATE)\n",
        "RESIDENCE (PER–LOC) \n",
        "BIRTHPLACE (PER–LOC)\n",
        "NATIONALITY (PER–LOC)\n",
        "EMPLOYEE_OF (PER–ORG) \n",
        "EDUCATED_AT (PER–ORG) \n",
        "\n",
        "<br>\n",
        "\n",
        "Here are the steps involved in building a relation classifier and creating a subset of the KnowledgeNet data:\n",
        "\n",
        "**Load the KnowledgeNet dataset**. The KnowledgeNet dataset is a collection of annotated sentences, each of which has been linked to at least one entity in Wikidata. The entities are linked to the sentences using a variety of relations.\n",
        "\n",
        "**Preprocess the data**. The data needs to be preprocessed before it can be used to train a model. This includes cleaning the data, removing stop words, and tokenizing the sentences.\n",
        "\n",
        "**Create a training set and a test set**. The data needs to be split into a training set and a test set. The training set will be used to train the model, and the test set will be used to evaluate the performance of the model.\n",
        "\n",
        "**Choose a machine learning algorithm.** There are a variety of machine learning algorithms that can be used to build a relation classifier. Some of the most common algorithms include support vector machines, naive Bayes, and logistic regression.\n",
        "\n",
        "**Train the model.** The model needs to be trained on the training set. This involves feeding the model the training data and allowing it to learn the relationships between the entities and the relations.\n",
        "\n",
        "**Evaluate the model.** The model needs to be evaluated on the test set to determine its performance. This involves feeding the model the test data and measuring its accuracy.\n",
        "\n",
        "**Use the model to extract relations from the KnowledgeNet data.** Once the model has been trained and evaluated, it can be used to extract relations from the KnowledgeNet data. This involves feeding the model the KnowledgeNet data and extracting the relations that the model has learned to recognize.\n",
        "Here are the steps involved in creating a subset of the KnowledgeNet data:\n",
        "\n",
        "1. **Load the KnowledgeNet dataset.**\n",
        "2. **Filter the data to include only sentences that contain the desired relations.**\n",
        "3. **Write the filtered data to a new file.**\n",
        "\n",
        "Here is an example of how to create a subset of the KnowledgeNet data that contains only sentences that contain the relations DATE_OF_BIRTH, RESIDENCE, BIRTHPLACE, NATIONALITY, EMPLOYEE_OF, and EDUCATED_AT:\n",
        "\n",
        "import json\n",
        "\n",
        "**Load the KnowledgeNet dataset.**\n",
        "with open('train.json', 'r') as f:\n",
        "knowledge_net = json.load(f)\n",
        "\n",
        "**Filter the data to include only sentences that contain the desired relations.**\n",
        "filtered_knowledge_net = []\n",
        "for sentence in knowledge_net:\n",
        "for relation in ['DATE_OF_BIRTH', 'RESIDENCE', 'BIRTHPLACE', 'NATIONALITY', 'EMPLOYEE_OF', 'EDUCATED_AT']:\n",
        "if relation in sentence['relations']:\n",
        "filtered_knowledge_net.append(sentence)\n",
        "\n",
        "**Write the filtered data to a new file.**\n",
        "with open('subset.json', 'w') as f:\n",
        "json.dump(filtered_knowledge_net, f, indent=4)\n",
        "\n",
        "This code will create a new file called subset.json that contains a subset of the KnowledgeNet data that contains only sentences that contain the relations DATE_OF_BIRTH, RESIDENCE, BIRTHPLACE, NATIONALITY, EMPLOYEE_OF, and EDUCATED_AT.\n"
      ],
      "metadata": {
        "id": "6PbwTxc5lW_a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT-dUtq3lMRX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the KnowledgeNet dataset.\n",
        "with open('train.json', 'r') as f:\n",
        "    knowledge_net = json.load(f)\n",
        "\n",
        "# Filter the data to include only sentences that contain the desired relations.\n",
        "filtered_knowledge_net = []\n",
        "for sentence in knowledge_net:\n",
        "    for relation in ['DATE_OF_BIRTH', 'RESIDENCE', 'BIRTHPLACE', 'NATIONALITY', 'EMPLOYEE_OF', 'EDUCATED_AT']:\n",
        "        if relation in sentence['relations']:\n",
        "            filtered_knowledge_net.append(sentence)\n",
        "\n",
        "# Create a training set and a test set.\n",
        "train_set = filtered_knowledge_net[:int(len(filtered_knowledge_net) * 0.8)]\n",
        "test_set = filtered_knowledge_net[int(len(filtered_knowledge_net) * 0.8):]\n",
        "\n",
        "# Create a TF-IDF vectorizer.\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Create a logistic regression model.\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model.\n",
        "model.fit(vectorizer.fit_transform([sentence['text'] for sentence in train_set]), [relation for sentence in train_set for relation in sentence['relations']])\n",
        "\n",
        "# Evaluate the model.\n",
        "predictions = model.predict(vectorizer.transform([sentence['text'] for sentence in test_set]))\n",
        "accuracy = np.mean(predictions == [relation for sentence in test_set for relation in sentence['relations']])\n",
        "\n",
        "print('Accuracy:', accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Read the JSON file into a Pandas DataFrame.\n",
        "df = pd.read_json('https://raw.githubusercontent.com/diffbot/knowledge-net/master/dataset/train.json')\n",
        "\n",
        "# Extract the entities and relations from the DataFrame.\n",
        "entities = df['entities'].values\n",
        "relations = df['relations'].values\n",
        "\n",
        "# Create a training and testing set.\n",
        "X_train, X_test, y_train, y_test = train_test_split(entities, relations, test_size=0.2)\n",
        "\n",
        "# Create a TfidfVectorizer.\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Transform the training and testing sets.\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Create a LogisticRegression classifier.\n",
        "clf = LogisticRegression()\n",
        "\n",
        "# Train the classifier.\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluate the classifier on the testing set.\n",
        "y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "# Print the accuracy of the classifier.\n",
        "print(clf.score(X_test_tfidf, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "7F6HRAEmeGNB",
        "outputId": "2dfcbc3a-a310-4539-ceed-3c06a54a21d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ea88d6718385>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Read the JSON file into a Pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/diffbot/knowledge-net/master/dataset/train.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Extract the entities and relations from the DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1321\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             )\n\u001b[1;32m   1323\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Trailing data"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Create a Knowledge Graph that can store the information contained in these sentences. You can use any open-source graph database for this purpose. \n",
        "\n",
        "Here are the steps involved in creating a Knowledge Graph that can store the information contained in the sentences:\n",
        "\n",
        "Choose a graph database. There are a variety of open-source graph databases available, such as Neo4j, ArangoDB, and OrientDB. Choose a graph database that meets your needs and requirements.\n",
        "Create a schema for the Knowledge Graph. The schema defines the structure of the Knowledge Graph. It specifies the types of nodes and edges that are allowed in the Knowledge Graph.\n",
        "Load the data into the Knowledge Graph. The data can be loaded into the Knowledge Graph using a variety of methods, such as Cypher queries, LOAD CSV statements, and Bulk API calls.\n",
        "Query the Knowledge Graph. The Knowledge Graph can be queried using a variety of methods, such as Cypher queries, SPARQL queries, and GraphQL queries.\n",
        "Visualize the Knowledge Graph. The Knowledge Graph can be visualized using a variety of tools, such as Neo4j Bloom, ArangoDB Graph Studio, and OrientDB Graph Inspector.\n",
        "Here is an example of how to create a Knowledge Graph using Neo4j:"
      ],
      "metadata": {
        "id": "EsAvSgL5msZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import py2neo\n",
        "\n",
        "# Create a connection to the Neo4j database.\n",
        "graph = py2neo.Graph()\n",
        "\n",
        "# Create a schema for the Knowledge Graph.\n",
        "graph.schema.create_node_type('Person')\n",
        "graph.schema.create_node_type('Organization')\n",
        "graph.schema.create_relationship_type('BornIn', ['Person', 'Location'])\n",
        "graph.schema.create_relationship_type('ResidesIn', ['Person', 'Location'])\n",
        "graph.schema.create_relationship_type('WorksFor', ['Person', 'Organization'])\n",
        "graph.schema.create_relationship_type('StudiedAt', ['Person', 'Organization'])\n",
        "\n",
        "# Load the data into the Knowledge Graph.\n",
        "with open('train.json', 'r') as f:\n",
        "    for sentence in json.load(f):\n",
        "        for relation in sentence['relations']:\n",
        "            if relation == 'DATE_OF_BIRTH':\n",
        "                graph.create(Person(name=sentence['subject'], birth_date=sentence['object']))\n",
        "            elif relation == 'RESIDENCE':\n",
        "                graph.create(Person(name=sentence['subject'], residence=sentence['object']))\n",
        "            elif relation == 'BIRTHPLACE':\n",
        "                graph.create(Person(name=sentence['subject'], birthplace=sentence['object']))\n",
        "            elif relation == 'NATIONALITY':\n",
        "                graph.create(Person(name=sentence['subject'], nationality=sentence['object']))\n",
        "            elif relation == 'EMPLOYEE_OF':\n",
        "                graph.create(Person(name=sentence['subject'], employer=sentence['object']))\n",
        "            elif relation == 'EDUCATED_AT':\n",
        "                graph.create(Person(name=sentence['subject'], university=sentence['object']))\n",
        "\n",
        "# Query the Knowledge Graph.\n",
        "# Find all people who were born in the United States.\n",
        "query = '''\n",
        "MATCH (p:Person)\n",
        "WHERE p.birth_date > '1776-07-04'\n",
        "RETURN p.name\n",
        "'''\n",
        "results = graph.cypher.execute(query)\n",
        "for result in results:\n",
        "    print(result['p.name'])\n",
        "\n",
        "# Visualize the Knowledge Graph.\n",
        "# Use Neo4j Bloom to visualize the Knowledge Graph.\n",
        "from neo4j_bloom import Graph\n",
        "\n",
        "graph = Graph('bolt://localhost:7687', 'neo4j', 'password')\n",
        "\n",
        "# Create a visualization of the Knowledge Graph.\n",
        "graph.create_visualization('my_graph')\n",
        "\n",
        "# Open the visualization in a web browser.\n",
        "graph.open_visualization('my_graph')\n"
      ],
      "metadata": {
        "id": "Tmd2mIuLoy9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, I can do that. Here are the steps involved in connecting a Knowledge Graph to a front end that can take in Natural Language Queries and give the answers back:\n",
        "\n",
        "**Choose a chatbot framework. **There are a variety of open-source chatbot frameworks available, such as Rasa, Dialogflow, and Amazon Lex. Choose a chatbot framework that meets your needs and requirements.\n",
        "\n",
        "**Create a chatbot model.** The chatbot model is a machine learning model that can understand and respond to Natural Language Queries. The model can be trained on a variety of data, such as text corpora, dialogue transcripts, and question-answering datasets.\n",
        "\n",
        "**Connect the chatbot model to the Knowledge Graph.** The chatbot model can be connected to the Knowledge Graph using a variety of methods, such as REST APIs, GraphQL APIs, and WebSockets.\n",
        "\n",
        "**Deploy the chatbot.** The chatbot can be deployed to a variety of platforms, such as a web server, a mobile app, or a voice assistant.\n",
        "\n",
        "Here is an example of how to connect a Knowledge Graph to a chatbot using Rasa:\n",
        "\n",
        "**1. Create a Rasa project.** The Rasa project is a collection of files that define the chatbot model and the chatbot configuration.\n",
        "\n",
        "**2. Add the Knowledge Graph to the Rasa project.** The Knowledge Graph can be added to the Rasa project by adding the following lines to the domain.yml file:\n",
        "\n",
        "\n",
        "Code snippet :\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "knowledge_graph:\n",
        "  url: \"bolt://localhost:7687\"\n",
        "  username: \"neo4j\"\n",
        "  password: \"password\"\n",
        "```\n",
        "\n",
        "**Train the chatbot model.** The chatbot model can be trained on a dataset of Natural Language Queries and their corresponding answers.\n",
        "\n",
        "**Deploy the chatbot.** The chatbot can be deployed to a variety of platforms, such as a web server, a mobile app, or a voice assistant.\n",
        "\n",
        "Once the chatbot is deployed, it can be used to take in Natural Language Queries and give the answers back. The chatbot can also be used to continue a conversation rather than only Question-Answering."
      ],
      "metadata": {
        "id": "1bpFYzwopPjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rasa\n",
        "import py2neo\n",
        "\n",
        "# Create a Rasa project.\n",
        "project = rasa.core.projects.create('my_project')\n",
        "\n",
        "# Add the Knowledge Graph to the Rasa project.\n",
        "knowledge_graph = py2neo.Graph('bolt://localhost:7687', 'neo4j', 'password')\n",
        "project.domain.add_knowledge_graph(knowledge_graph)\n",
        "\n",
        "# Train the chatbot model.\n",
        "project.train()\n",
        "\n",
        "# Deploy the chatbot.\n",
        "project.deploy()\n",
        "\n",
        "# Use the chatbot.\n",
        "# Create a conversation.\n",
        "conversation = rasa.core.agent.get_conversation(project, 'default')\n",
        "\n",
        "# Start the conversation.\n",
        "conversation.start()\n",
        "\n",
        "# Send a message to the chatbot.\n",
        "message = 'What is the capital of France?'\n",
        "\n",
        "conversation.send(message)\n",
        "\n",
        "# Get the chatbot's response.\n",
        "response = conversation.receive()\n",
        "\n",
        "# Print the chatbot's response.\n",
        "print(response.text)\n",
        "\n",
        "# End the conversation.\n",
        "conversation.end()\n"
      ],
      "metadata": {
        "id": "26mzj8MFqMxp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}